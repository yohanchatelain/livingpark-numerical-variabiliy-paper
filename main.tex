\section{Introduction}

The reliability of MRI measures critically depends on the estimation of all
sources of analytical
variability~\cite{carp2012plurality,botvinik2020variability,kennedy2019everything}.
Estimating variability is particularly important when effect sizes are moderate,
as it is the case in studies of Parkinson's disease
(PD)~\cite{he2020progressive}. Among variability sources, numerical
variability---differences in computational results arising from factors such as
hardware, operating systems, or software library versions---has been shown to
have a measurable impact on analyses across multiple pipeline
tools~\cite{glatard2012virtual,gronenschild2012effects,des2023reproducibility,vila2024impact}.

Numerical variability arises from rounding and truncation errors associated with
the use of limited-precision numerical formats, such as the widespread IEEE-754
standard for floating-point arithmetic~\cite{markstein2008new}. Due to the
complexity of MRI analyses, particularly their reliance on high-dimensional
optimization, such errors can propagate and accumulate, sometimes leading to
measurable differences in final
outputs~\cite{salari2021accurate,kiar2021numerical,chatelain2024numerical,mirhakimi2025numerical}.
However, numerical variability remains understudied, mainly due to the practical
challenges of quantifying its effects. Consequently, the implications of
numerical variability for clinical findings are largely unknown.

The first aim of this study was to quantify the impact of numerical variability
in structural MRI analyses of Parkinson's disease. Building on these initial
observations, we developed an analytical framework and associated tools to
rapidly assess the numerical quality of structural MRI analyses reported in the
published literature. By making numerical-variability evaluation accessible, our
framework and tool enhance transparency, support peer review, and promote more
reliable statistical inference in neuroimaging. Applying this framework to the
PD literature, we provide the first estimates of the numerical quality of MRI
analyses in Parkinson's disease studies, showing that \ldots \TG{revise last
    sentence when results are available.}

\section{Results}

\subsection{Numerical variability alters statistical inference in MRI measures of PD}

We assessed the impact of numerical variability on conclusions
drawn from MRI analyses of Parkinson's disease, focusing on two common
analyses: (1) volumetric group differences between PD subjects and
Healthy Controls (HC), and (2) partial correlations between regional volumes and
motor evaluation scores measured with the MDS-Unified Parkinson's Disease
Rating Scale part 3 (UPDRS-III). For both, we conducted a cross-sectional
analysis at baseline and a longitudinal analysis across two time points.

We selected T1-weighted MRI data from the Parkinson Progression Marker
Initiative (PPMI) dataset~\cite{marek2011parkinson}, including participants with
at least two usable visits separated by 0.9-2.0 years, and excluding
participants with Mild Cognitive Impairment (MCI) or other neurological
disorders. The final dataset included  112 PD participants without MCI
(PD-non-MCI) and 89 HC participants (Table~\ref{tab:cohort_stat}). All images
underwent standard pre-processing using FreeSurfer's longitudinal stream:
cross-sectional processing of both timepoints, followed by creation of an
unbiased within-subject template using robust registration. We introduced
numerical noise mimicking realistic perturbations into this pipeline using Monte
Carlo Arithmetic (MCA)~\cite{parker1997monte}, a technique that injects random,
zero-mean perturbations into floating-point operations while perserving
mathematical expectations. We repeated the perturbed analyses, yielding 26
usable runs, to estimate numerical variability. We verified the validity of the
numerical perturbation approach by confirming that all unperturbed results
belong to the range defined by numericall perturbed results.

For both group and correlation analyses, statistical outcomes varied
substantially across the 26 Monte Carlo Arithmetic (MCA) repetitions (Figures
\ref{fig:significance_correlation_subcortical_volume}-\ref{fig:significance_correlation_thickness}).
In subcortical volumes (14 regions; Figure
\ref{fig:significance_correlation_subcortical_volume}), significance flipped in
27\% of regions \TG{of the regions or of the (region, analysis) pairs?},
indicating frequent inconsistencies across repetitions. In cortical thickness
(68 regions; Figure \ref{fig:significance_correlation_thickness}), 21\% of
regions \TG{same comments} were similarly found to be unstable. These
fluctuations demonstrate the measurable impact of numerical noise on structural
MRI measures of PD.

\begin{figure}
    \includegraphics[width=\linewidth]{figures/consistency/subcortical_volume_significance_correlation.pdf}
    \caption{ Proportion of significant tests ($p<0.05$) for subcortical volumes
        across 26 numerical perturbations. measures \TG{could you sort the
        x-axis by region first rather than by
        hemisphere?}.\label{fig:significance_correlation_subcortical_volume}
        \TG{Make sure that ``right accumbens area'' also appears in the x
        labels}}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/consistency/cortical_thickness_significance_correlation.pdf}
    \caption{Proportion of significant tests ($p<0.05$) for cortical thickness
        across 26 numerical perturbations. measures.\label{fig:significance_correlation_thickness}}
    \label{fig:navr_consistency_thickness_plot}
\end{figure}


\subsection{A practical model to quantify the impact of numerical variability}
% Comments:
% - Build a tool that can be broadly applied to any neuroimaging
% - Analytical modeling of sigma_d
% - Explain why having a tool is important
% - Why having a tool fast is important
%   - Measuring numerical variability is a time-consuming process
%   - Analytical modeling of sigma_d allows applying the tool to any
%     neuroimaging papers, existing results.
% - Quality Control impact findings => a tool to find potentially unreliable
%   results
% - To be general, we developped an analytical model
% - Refers to the online tool on yohanchatelain.github.io/brain-render

% \TG[done]{Instead of re-stating the importance of numerical variability in this
%     paragraph, which supposedly should be understood from the end of the
%     introduction, here you could explain the need for a tool to quickly and
%     practically evaluate its impact in a given study, explain and justify your
%     assumptions (e.g., numerical variability is a feature of the pipeline rather
%     than the population---refer to previous works). In doing so, you could explain
%     how running MCA is currently not realistic due to computational requirements
%     (although new architectures may enable it in the coming years), and explain the
%     need for a statistical correction.}

The previous results underscore the importance of quantifying the impact of
numerical variability in MRI analyses, as such variability can substantially
affect statistical inference. However, conducting such empirical evaluations of
numerical variability routinely is impractical due to their substantial
computational demands. We developed an analytical model of numerical variability
in structural MRI analyses by deriving closed-form approximations of numerical
uncertainty for common statistics and their associated p-values
(Table~\ref{tab:stat_uncertainty}), which can be readily integrated in
statistical analyses. The main variable of interest is the
Numerical-Population Variability Ratio (\navr) that represents the relative
magnitude of numerical variability ($\sigma_{\mathrm{num}}$;
Eq.~\eqref{eq:sigma_num}) to population variability ($\sigma_{\mathrm{pop}}$;
Eq.~\eqref{eq:sigma_anat}):
$$\nu_{\mathrm{npv}} = \frac{\sigma_{\mathrm{num}}}{\sigma_{\mathrm{pop}}}$$
The \navr ratio quantifies the numerical instability of a neuroimaging analysis,
allowing comparison across regions and studies. The formulas in
Table~\ref{tab:stat_uncertainty} were obtained by propagating the numerical
variability through the estimators using the delta-method (see
Section~\ref{sec:theoretical_derivations}) and were verified through numerical
simulations.

This analytical framework provides a practical link between the pipeline's
numerical instability (\navr), the study's sample size ($n$), and the
resulting uncertainty in effect sizes and $p$-values. Because the formulas
rely only on the study's summary statistics, they can be used to
assess the potential impact of numerical variability on existing
studies without requiring costly recomputation or access to the original data.

% \TG[done]{I think I would rather present the $\sigma_d$ result first, then explain
%     NAV. When introducing sigma d, you could better explain that numerical
%     variability has to be evaluated in the context of a particular effect size, and its impact will be dependent on sample size.}

\begin{table}[ht]
    \centering
    \begin{tabular}{lll}
        \toprule
        \textbf{Statistic}
                            & \textbf{Numerical standard deviation}
                            & \textbf{Numerical p-value uncertainty}                                                                                                           \\
        \midrule
        Cohen's $d$         & $\sigma_d \approx\nu_{\mathrm{npv}}\frac{2}{\sqrt{n}}$             & \multicolumn{1}{c}{-}                                                       \\
        Two-sample $t$      & $\sigma_t \approx \nu_{\mathrm{npv}}        $                      & $\sigma_{p} \approx 2f_{t,df}(|t|)\nu_{\mathrm{npv}}$                       \\
        Partial correlation & $\sigma_r \geq \nu_{\mathrm{npv}}\sqrt{\frac{(1-r^{2})^{3}}{n-1}}$ & $\sigma_{p} \geq 2f_{t,df}(|t|)\sqrt{\frac{df}{n-1}}\nu_{\mathrm{npv}}$     \\
        ANCOVA              & $\sigma_F \approx 2\sqrt{F}\nu_{\mathrm{npv}}$                     & $\sigma_{p} \approx 2\sqrt{F}f_{\mathcal{F}}(F;1,df_2)\,\nu_{\mathrm{npv}}$ \\
        \bottomrule
    \end{tabular}
    \caption{First-order numerical uncertainty of common statistical tests under
        Monte Carlo Arithmetic perturbations. Cohen's d formula assumes large
        and equal group sizes. $f_{t,df}$ and $f_\mathcal{F}(F;1,df_2)$ denote
        the probability density functions of the Student's $t$-distribution with
        $df$ degrees of freedom and the $\mathcal{F}$-distribution with $(1,
            df_2)$ degrees of freedom, respectively. The $p$-value approximation for
        the partial correlation uses $t=r(df/(1-r^2))^{1/2}$.}
    \label{tab:stat_uncertainty}
\end{table}

We measured $\nu_{npv}$ in the PPMI dataset used in the previous section
(Figure~\ref{fig:navr}). For the baseline analysis, the average \navr value
across cortical and subcortical regions was \TG{add value} in the PD group and
\TG{value} in the HC group. No significant differences in mean \navr were
observed between HC and PD groups (Bootstrap with 10,000 samples), supporting
the idea that numerical variability measured through \navr may roughly
generalize across populations, for structural MRI measures produced by
FreeSurfer. The Cohen's d formula in Table~\ref{tab:stat_uncertainty} gives that
for this level of uncertainty, achieving a negligible level of numerical
uncertainty ($\sigma_d \leq 0.01$) would require \TG{100} participants.
Longitudinal analyses exhibited substantially higher \navr values, \TG{report
    averages}, likely owing to the occurrence of catastrophic cancellation when
substracting nearly equal numbers between timepoints. Using the same formula, we
anticiapte that at this level of uncertainty, achieving a negligible numerical
uncertainty ($\sigma_d \leq 0.01$) would require at least 1,600 participants.

To facilitate the use of our model, we developed an interactive web tool available at
\href{https://yohanchatelain.github.io/brain\_render/}{yohanchatelain.github.io/brain\_render}.
(Figure~\ref{fig:brain_render_tool}).

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/NAVR_map/NAVR_thickness_subcortical_volumes.png}
    \caption{Numerical-Population Variability Ratio (\navr) for subcortical volumes (top row in each panel) and cortical
        thickness (bottom row in each panel) in healthy controls (HC) and patients with
        Parkinson's disease (PD). Panels show \navr maps for HC at baseline (a), PD
        at baseline (b), HC longitudinally between follow-up and baseline scans
        (c), and PD longitudinally (d). Higher \navr values indicate greater
        computational uncertainty relative to inter-subject variability.
        Warmer colors correspond to higher \navr values.}
    \label{fig:navr}
\end{figure}

% \begin{figure}
%     \includegraphics[width=\linewidth]{figures/sigma_d_contour.pdf}
%     \caption{\TG{could you show some of the papers in this figure? } Relationship between \navr and population sample size  \(N\) for
%         predicting the uncertainty in Cohen's d effect size estimation. The
%         contour lines represent different \navr values, showing how numerical
%         variability scales with sample size. With a typical \navr value of 0.2,
%         to maintain reliable effect size estimates $\sigma_d \leq 0.01$, the
%         plot suggests to use $N \geq 1500$.\label{fig:sigma_d_contour}}
% \end{figure}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/screenshot_light.png}
    \caption{Interactive web tool for estimating NPVR and assessing numerical
        variability in neuroimaging studies. Users can input summary statistics
        to obtain NPVR values and visualize the impact of numerical variability
        on effect size estimates. The tool is available at
        \href{https://yohanchatelain.github.io/brain\_render/}{yohanchatelain.github.io/brain\_render}.}
    \label{fig:brain_render_tool}
\end{figure}

\subsection{Impact of numerical variability on published findings}

To assess the broader implications of numerical variability, we applied numerical uncertainty propagation (Table~\ref{tab:stat_uncertainty}) to re-evaluate findings from ten representative articles on Parkinson's disease. These studies were selected to illustrate the potential impact of numerical variability on reported outcomes based on available summary statistics. Applying \navr-based thresholding to published p-values identified multiple instances where the statistical significance of reported findings was unstable.

For each p-value reported as significant in the original articles, we calculated the associated numerical uncertainty using the formulae in Table~\ref{tab:stat_uncertainty}. We then estimated the probability of a significance flip where a result transitions from significant to non-significant due to numerical error by modeling the true p-value as a Beta distribution. This distribution is parameterized such that the reported p-value represents the mean and the calculated numerical uncertainty represents the standard deviation. By computing the cumulative distribution function of this characterized Beta distribution at the original significance threshold, we derived the probability that the reported significant finding was, in fact, a numerically induced false positive.

Figure~\ref{fig:false_positive_vs_sample_size} displays the probability of such false positives as a function of sample size across different effect size metrics: T-values (T), F-values (F), and correlation coefficients (R). Our analysis yields four key observations. First, no specific statistical test appears inherently more robust to numerical variability than others. Second, contrary to standard statistical intuition regarding sampling error, sample size does not appear to mitigate the probability of numerically induced false positives; for example, cross-sectional volume analyses exhibited similar instability magnitudes at sample sizes of both 43 and 315. 

Third, while longitudinal analyses generally showed lower false positive probabilities than cross-sectional analyses, they simultaneously exhibited higher \navr. This apparent contradiction suggests that the proximity of a p-value to the significance threshold is the primary driver of instability: p-values hovering near the threshold are highly sensitive to numerical noise, regardless of the study design. Finally, cortical measures demonstrated higher false positive probabilities compared to subcortical measures, indicating a greater sensitivity to numerical variability in cortical measures.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/false_positive_vs_sample_size_boxplots.pdf}
    \caption{Probability of a numerically induced false positive (significance flip) as a function of sample size across different effect sizes: T-values (T), F-values (F), and correlation coefficients (R). Probabilities are derived from the overlap between the numerical uncertainty distribution (Beta distribution) and the significance threshold.}
    \label{fig:false_positive_vs_sample_size}
\end{figure}

\section{Discussion}

% Comments:
% 1. Summarize the main results and what do they mean
% 2. Extension beyond FreeSurfer and expectation to generalize the findings to other neuroimaging software
% 3. Discuss the potential sources of numerical variability (minimal local, minimal precision, etc.)

% [] Mention using neuroimaging as biomarker (n=1 scenario), personalized medicine,

Our systematic perturbation of FreeSurfer revealed that numerical variability
alone can account for up to 30\% \TG{check value, can we report the average
    $\nu_{nav}$ observed in PD participants?} of the population variability observed
in structural MRI measurements. This level of uncertainty can significantly
impact statistical outcomes in Parkinson's disease analyses, leading to the
appearance or disappearance of group differences or correlations depending
solely on computational conditions. These findings offer a mechanistic
explanation for some of the reproducibility challenges reported in clinical
neuroimaging.

To facilitate numerical evaluations in previous and future studies, we
introduced the Numerical-Population Variability Ratio (NPVR), a quantitative
framework for assessing the relative magnitude of computational noise. By
establishing a theoretical link between NPVR and the uncertainty in common
statistics, we provided a practical tool to assess the numerical robustness of MRI measures.
Our re-analysis of published ENIGMA results illustrates this utility: while
large sample sizes confer robustness to core findings, many secondary effects
fall below the computational noise floor. This suggests that in exploratory
studies with lower sample size, numerical instability may undermine the
reliability of reported effects. \TG{revise this part when results are available}

Although our primary analysis focused on FreeSurfer 7.3.1 and Parkinson's
disease, the underlying numerical issues are more general.
Previous analyses of FSL~\cite{mirhakimi2025numerical} and ANTs~\YC{cite
    Mathieu} indicate that such instability is not unique to FreeSurfer but likely
pervades the field. SPM however seems to be less impacted by numerical
variability, possibly due to the use of Bayesian optimization~\cite{mirhakimi2025numerical}.

Traditional image processing methods rely on nonlinear optimization
procedures that can converge to different local minima under small
perturbations, resulting in substantive changes to derived measures. To address
this problem and decrease the execution time, the neuroimaging field is
increasingly shifting toward deep learning models. The latest FreeSurfer
release (v8), for example, now incorporates Deep Learning (DL) models such as
FastSurfer~\cite{henschel2020fastsurfer} and
Synthmorph~\cite{hoffmann2021synthmorph} to replace its classical segmentation
and registration steps. However, this shift does not eliminate the problem of
instability but rather reframes it. While these DL models have been shown
stable during the inference stage~\cite{pepe2023numerical}, their training
process is subject to its own sources of variability. Factors like weight
initialization and floating-point precision can cause different training runs
to yield distinct models with varying performance, and their quantification
remains an open question in neuroimaging. This phenomenon is analogous to the
local minima issue in classical optimization. Ultimately, whether arising from
classical optimization or DL training, such instability means that even
identical inputs can lead to divergent interpretations, raising critical
concerns for research reproducibility and clinical translation. \TG{Reword paragraph to refer to Ines' latest results on FastSurfer training}

To address the generalizability of our findings, we considered the
characteristics of our data cohorts. A potential limitation is that our
Parkinson's Disease cohort was relatively homogeneous in age and phenotype,
which could reduce population variance and consequently inflate NPVR values.
However, direct statistical analyses revealed no significant differences in
numerical variability between the PD and healthy control groups (see
Supplementary Fig. X). This key finding supports the idea that the pipeline's
instability is a consistent factor and that our results are likely to generalize
across these populations.\TG[done]{I would moderate this statement a little bit.
    I think it would be interesting the measure the NAV in different pipelines and
    datasets, but I don't think there's a very strong need for that.} Measuring the
NPVR across more diverse datasets, software packages, and disease contexts would
nevertheless be a valuable step. Such work would help build a more comprehensive
map of computational reliability across the entire neuroimaging landscape.

NPVR provides a scalable, interpretable metric to quantify hidden numerical
variability. Although floating-point rounding is a dominant source of
instability, future work should broaden this analysis to other contributors,
including algorithmic choices, preprocessing decisions, and data handling
practices. A comprehensive understanding of these factors is essential for
developing numerically robust software. Our results show that computational
uncertainty is as critical as statistical uncertainty in neuroimaging;
systematic assessments of numerical variability, exemplified by NPVR, are
therefore necessary to ensure the reproducibility and reliability of
neuroimaging-based biomarkers. Extending this quantification to the
deep-learning training stage is equally important, given the field's central
role in modern neuroimaging, and would support more robust and interpretable
models. Likewise, evaluating numerical variability in the classical
optimization schemes used in non-linear registration is a key milestone, as
these traditional tools often provide the reference for training deep-learning
models. Advancing both lines of analysis will benefit conventional and
learning-based approaches alike. \TG[done]{I would merge this paragraph with
    the previous one, and highlight specific next steps regarding numerical
    variability in neuroimaging.}
